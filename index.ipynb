{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import deque\n",
    "\n",
    "import numpy as np\n",
    "import torch as th\n",
    "\n",
    "import gym\n",
    "from gym.spaces import Box, Discrete\n",
    "\n",
    "from gfootball.env import create_environment\n",
    "from gfootball.env import observation_preprocessing\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.ppo import CnnPolicy\n",
    "from stable_baselines3.common import results_plotter\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.vec_env.dummy_vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.vec_env.subproc_vec_env import SubprocVecEnv\n",
    "\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import base64\n",
    "import pickle\n",
    "import zlib\n",
    "import gym\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch as th\n",
    "from torch import nn, tensor\n",
    "from collections import deque\n",
    "from gym.spaces import Box, Discrete\n",
    "from gfootball.env import create_environment, observation_preprocessing\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.ppo import CnnPolicy\n",
    "from stable_baselines3.common import results_plotter\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
    "from stable_baselines3.common.vec_env.dummy_vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.vec_env.subproc_vec_env import SubprocVecEnv\n",
    "from IPython.display import HTML\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenarios = {0: \"academy_empty_goal_close\",\n",
    "             1: \"academy_empty_goal\",\n",
    "             2: \"academy_run_to_score\",\n",
    "             3: \"academy_run_to_score_with_keeper\",\n",
    "             4: \"academy_pass_and_shoot_with_keeper\",\n",
    "             5: \"academy_run_pass_and_shoot_with_keeper\",\n",
    "             6: \"academy_3_vs_1_with_keeper\",\n",
    "             7: \"academy_corner\",\n",
    "             8: \"academy_counterattack_easy\",\n",
    "             9: \"academy_counterattack_hard\",\n",
    "             10: \"academy_single_goal_versus_lazy\",\n",
    "             11: \"11_vs_11_kaggle\"}\n",
    "scenario_name = scenarios[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FootballGym(gym.Env):\n",
    "    spec = None\n",
    "    metadata = None\n",
    "    \n",
    "    def __init__(self, config=None):\n",
    "        super(FootballGym, self).__init__()\n",
    "        env_name = \"academy_empty_goal_close\"\n",
    "        rewards = \"scoring,checkpoints\"\n",
    "        if config is not None:\n",
    "            env_name = config.get(\"env_name\", env_name)\n",
    "            rewards = config.get(\"rewards\", rewards)\n",
    "        self.env = create_environment(\n",
    "            env_name=env_name,\n",
    "            stacked=False,\n",
    "            representation=\"raw\",\n",
    "            rewards = rewards,\n",
    "            write_goal_dumps=False,\n",
    "            write_full_episode_dumps=False,\n",
    "            render=False,\n",
    "            write_video=False,\n",
    "            dump_frequency=1,\n",
    "            logdir=\".\",\n",
    "            extra_players=None,\n",
    "            number_of_left_players_agent_controls=1,\n",
    "            number_of_right_players_agent_controls=0)  \n",
    "        self.action_space = Discrete(19)\n",
    "        self.observation_space = Box(low=0, high=255, shape=(72, 96, 16), dtype=np.uint8)\n",
    "        self.reward_range = (-1, 1)\n",
    "        self.obs_stack = deque([], maxlen=4)\n",
    "        \n",
    "    def transform_obs(self, raw_obs):\n",
    "        obs = raw_obs[0]\n",
    "        obs = observation_preprocessing.generate_smm([obs])\n",
    "        if not self.obs_stack:\n",
    "            self.obs_stack.extend([obs] * 4)\n",
    "        else:\n",
    "            self.obs_stack.append(obs)\n",
    "        obs = np.concatenate(list(self.obs_stack), axis=-1)\n",
    "        obs = np.squeeze(obs)\n",
    "        return obs\n",
    "\n",
    "    def reset(self):\n",
    "        self.obs_stack.clear()\n",
    "        obs = self.env.reset()\n",
    "        obs = self.transform_obs(obs)\n",
    "        return obs\n",
    "    \n",
    "    def step(self, action):\n",
    "        obs, reward, done, info = self.env.step([action])\n",
    "        obs = self.transform_obs(obs)\n",
    "        return obs, float(reward), done, info\n",
    "    \n",
    "check_env(env=FootballGym(), warn=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing.connection import Pipe\n",
    "import numpy as np\n",
    "from stable_baselines3 import PPO\n",
    "# from stable_baselines3.common.policies import CnnPolicy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.vec_env import VecEnv\n",
    "from pathos.multiprocessing import ProcessPool\n",
    "\n",
    "def worker(remote, parent_remote, env_fn_wrapper):\n",
    "    parent_remote.close()\n",
    "    env = env_fn_wrapper()\n",
    "    while True:\n",
    "        cmd, data = remote.recv()\n",
    "        if cmd == 'step':\n",
    "            ob, reward, done, info = env.step(data)\n",
    "            if done:\n",
    "                ob = env.reset()\n",
    "            remote.send((ob, reward, done, info))\n",
    "        elif cmd == 'reset':\n",
    "            ob = env.reset()\n",
    "            remote.send(ob)\n",
    "        elif cmd == 'close':\n",
    "            remote.close()\n",
    "            break\n",
    "        elif cmd == 'get_spaces':\n",
    "            remote.send((env.observation_space, env.action_space))\n",
    "        elif cmd == 'env_method':\n",
    "            method_name, method_args, method_kwargs = data\n",
    "            method = getattr(env, method_name)\n",
    "            remote.send(method(*method_args, **method_kwargs))\n",
    "        elif cmd == 'env_is_wrapped':\n",
    "            wrapper_class = data\n",
    "            remote.send(wrapper_class in env._env_is_wrapped_cache)\n",
    "        elif cmd == 'get_attr':\n",
    "            attr_name = data\n",
    "            remote.send(getattr(env, attr_name))\n",
    "        elif cmd == 'set_attr':\n",
    "            attr_name, value = data\n",
    "            setattr(env, attr_name, value)\n",
    "            remote.send(True)\n",
    "        elif cmd == 'seed':\n",
    "            seed = data\n",
    "            remote.send(env.seed(seed))\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "\n",
    "class CustomSubprocVecEnv(VecEnv):\n",
    "    def __init__(self, env_fns):\n",
    "        self.waiting = False\n",
    "        self.closed = False\n",
    "        n_envs = len(env_fns)\n",
    "        self.n_envs = n_envs\n",
    "        self.remotes, self.work_remotes = zip(*[Pipe() for _ in range(n_envs)])\n",
    "\n",
    "        self.pool = ProcessPool(n_envs)\n",
    "        self.ps = [self.pool.apipe(worker, work_remote, remote, env_fn)\n",
    "                   for (work_remote, remote, env_fn) in zip(self.work_remotes, self.remotes, env_fns)]\n",
    "\n",
    "        for remote in self.work_remotes:\n",
    "            remote.close()\n",
    "\n",
    "        self._dummy_env = DummyVecEnv([env_fns[0]])\n",
    "        super().__init__(self.n_envs, self._dummy_env.observation_space, self._dummy_env.action_space)\n",
    "\n",
    "\n",
    "\n",
    "    def step_async(self, actions):\n",
    "        for remote, action in zip(self.remotes, actions):\n",
    "            remote.send((\"step\", action))\n",
    "        self.waiting = True\n",
    "\n",
    "    def step_wait(self):\n",
    "        results = [remote.recv() for remote in self.remotes]\n",
    "        self.waiting = False\n",
    "        obs, rews, dones, infos = zip(*results)\n",
    "        return np.stack(obs), np.stack(rews), np.stack(dones), infos\n",
    "\n",
    "    def reset(self):\n",
    "        for remote in self.remotes:\n",
    "            remote.send((\"reset\", None))\n",
    "        return np.stack([remote.recv() for remote in self.remotes])\n",
    "\n",
    "    def close(self):\n",
    "        if self.closed:\n",
    "            return\n",
    "        if self.waiting:\n",
    "                remote.recv()\n",
    "        for remote in self.remotes:\n",
    "            remote.send((\"close\", None))\n",
    "        self.pool.close()\n",
    "        self.pool.join()\n",
    "        self.closed = True\n",
    "\n",
    "\n",
    "    def get_images(self):\n",
    "        for remote in self.remotes:\n",
    "            remote.send((\"render\", None))\n",
    "        imgs = [remote.recv() for remote in self.remotes]\n",
    "        return imgs\n",
    "\n",
    "    @property\n",
    "    def unwrapped(self):\n",
    "        return self._dummy_env.unwrapped\n",
    "    def env_method(self, method_name, *method_args, **method_kwargs):\n",
    "        for remote in self.remotes:\n",
    "            remote.send(('env_method', (method_name, method_args, method_kwargs)))\n",
    "        return [remote.recv() for remote in self.remotes]\n",
    "\n",
    "    def env_is_wrapped(self, wrapper_class):\n",
    "        for remote in self.remotes:\n",
    "            remote.send(('env_is_wrapped', wrapper_class))\n",
    "        return [remote.recv() for remote in self.remotes]\n",
    "\n",
    "    def get_attr(self, attr_name):\n",
    "        for remote in self.remotes:\n",
    "            remote.send(('get_attr', attr_name))\n",
    "        return [remote.recv() for remote in self.remotes]\n",
    "\n",
    "    def set_attr(self, attr_name, value):\n",
    "        for remote in self.remotes:\n",
    "            remote.send(('set_attr', (attr_name, value)))\n",
    "        return [remote.recv() for remote in self.remotes]\n",
    "\n",
    "    def seed(self, seed=None):\n",
    "        for remote in self.remotes:\n",
    "            remote.send(('seed', seed))\n",
    "        return [remote.recv() for remote in self.remotes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_env = CustomSubprocVecEnv([make_env(config, rank=i) for i in range(n_envs)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_env = FootballGym({\"env_name\":scenario_name})\n",
    "check_env(env=test_env, warn=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProgressBar(BaseCallback):\n",
    "    \n",
    "    def __init__(self, verbose=0):\n",
    "        super(ProgressBar, self).__init__(verbose)\n",
    "        self.pbar = None\n",
    "\n",
    "    def _on_training_start(self):\n",
    "        factor = np.ceil(self.locals['total_timesteps'] / self.model.n_steps)\n",
    "        try:\n",
    "            n = len(self.training_env.remotes)\n",
    "        except AttributeError:\n",
    "            n = len(self.training_env.envs)\n",
    "        total = int(self.model.n_steps * factor / n)\n",
    "        self.pbar = tqdm(total=total)\n",
    "\n",
    "    def _on_rollout_start(self):\n",
    "        self.pbar.refresh()\n",
    "\n",
    "    def _on_step(self):\n",
    "        self.pbar.update(1)\n",
    "        return True\n",
    "\n",
    "    def _on_rollout_end(self):\n",
    "        self.pbar.refresh()\n",
    "\n",
    "    def _on_training_end(self):\n",
    "        self.pbar.close()\n",
    "        self.pbar = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env(config=None, rank=0):\n",
    "    def _init():\n",
    "        env = FootballGym(config)\n",
    "        log_file = os.path.join(\".\", str(rank))\n",
    "        env = Monitor(env, log_file, allow_early_resets=True)\n",
    "        return env\n",
    "    return _init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_envs = 12\n",
    "n_steps = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env in a VecTransposeImage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ext3/miniconda3/envs/hpml/lib/python3.11/site-packages/stable_baselines3/ppo/ppo.py:137: UserWarning: You have specified a mini-batch size of 64, but because the `RolloutBuffer` is of size `n_steps * n_envs = 12`, after every 0 untruncated mini-batches, there will be a truncated mini-batch of size 12\n",
      "We recommend using a `batch_size` that is a factor of `n_steps * n_envs`.\n",
      "Info: (n_steps=1 and n_envs=12)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "config={\"env_name\":scenario_name}\n",
    "# train_env = DummyVecEnv([make_env(config, rank=i) for i in range(n_envs)])\n",
    "# train_env = SubprocVecEnv([make_env(config, rank=i) for i in range(n_envs)], start_method='spawn')\n",
    "train_env = CustomSubprocVecEnv([make_env(config, rank=i) for i in range(n_envs)])\n",
    "\n",
    "model = PPO(CnnPolicy, train_env, n_steps=n_steps, verbose=1)\n",
    "# model = PPO.load(\"../input/gfootball-stable-baselines3/ppo_gfootball.zip\", train_env)\n",
    "\n",
    "progressbar = ProgressBar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_timesteps = n_steps * n_envs * 500\n",
    "model.learn(total_timesteps=total_timesteps, callback=progressbar)\n",
    "model.save(\"ppo_gfootball\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "my_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
